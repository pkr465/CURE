# QGenie â€” C/C++ Codebase Update & Refactor Engine

`main.py` is the main entry point for a **multi-stage C/C++ codebase health analysis pipeline**. It is designed to analyze C/C++ codebases and produce rich health scores, structural metadata, and embeddings suitable for RAG (Retrieval-Augmented Generation) applications.

## Key Capabilities

1. **Static Analysis**: Uses a unified `StaticAnalyzerAgent` (7-phase pipeline) with 9 regex-based analyzers for fast health scoring.
2. **Deep Static Analysis Adapters**: Optional `--enable-adapters` mode powered by CCLS/libclang, Lizard, and Flawfinder for AST-accurate metrics.
3. **LLM-Powered Code Review**: `CodebaseLLMAgent` performs per-file semantic analysis and produces `detailed_code_review.xlsx`.
4. **Health Reporting**: Produces a canonical `healthreport.json` with metrics and summaries; optional HTML rendering.
5. **Data Flattening**: Converts reports to JSON and NDJSON formats for embedding.
6. **Vector DB Ingestion**: Ingests data into a **PostgreSQL** vector database with pgvector.
7. **Agentic Code Repair**: Human-in-the-loop `CodebaseFixerAgent` applies LLM-suggested fixes guided by reviewer feedback.
8. **Multi-Provider LLM**: Supports Anthropic Claude, QGenie, Google Vertex AI, and Azure OpenAI via `provider::model` format.
9. **Visualization**: Generates an HTML health report and provides a Streamlit UI dashboard.
10. **Telemetry & Analytics**: Silent PostgreSQL-backed telemetry tracks issues found/fixed, LLM usage, run durations, and fix success rates with a built-in dashboard.
11. **HITL Feedback Store**: PostgreSQL-backed persistent store for human feedback decisions and constraint rules, enabling agents to learn from accumulated human review history.

---

## Architecture & Workflow

### Standard Workflow (LangGraph)

The workflow is orchestrated using **LangGraph** and consists of four main agents:

1. **PostgreSQL Setup** (`postgres_db_setup_agent`): Sets up the schema and tables for vector storage.
2. **Codebase Analysis** (`codebase_analysis_agent`): Runs `StaticAnalyzerAgent` (7-phase pipeline with optional LLM enrichment and deep adapters). Generates `healthreport.json`.
3. **Flatten & NDJSON** (`flatten_and_ndjson_agent`): Flattens the report (`JsonFlattener`) and converts it to NDJSON (`NDJSONProcessor`) for embedding processing.
4. **Vector DB Ingestion** (`vector_db_ingestion_agent`): Ingests the processed records into PostgreSQL via `VectorDbPipeline`.

```text
PostgreSQL Setup
    â†“
Codebase Analysis (StaticAnalyzerAgent â€” 9 analyzers + optional deep adapters)
    â†“
Flatten & NDJSON
    â†“
Vector DB Ingestion
```

### Exclusive LLM Mode (`--llm-exclusive`)

Bypasses the LangGraph workflow entirely. Runs `CodebaseLLMAgent` for per-file semantic analysis producing `detailed_code_review.xlsx`. When combined with `--enable-adapters`, deep adapter results (complexity, security, dead code, call graph, function metrics) are merged as `static_*` tabs in the same Excel file.

```text
[Optional] Deep Static Adapters (Lizard, Flawfinder, CCLS)
    â†“
CodebaseLLMAgent (per-file LLM analysis)
    â†“
detailed_code_review.xlsx (LLM tabs + static_ adapter tabs)
```

### Deep Static Analysis Adapters (`--enable-adapters`)

When enabled, the following adapters run using real tooling instead of regex heuristics:

| Adapter                  | Backend       | Capabilities                                                |
| :----------------------- | :------------ | :---------------------------------------------------------- |
| `ASTComplexityAdapter`   | Lizard        | Real cyclomatic complexity, nesting depth, parameter counts |
| `SecurityAdapter`        | Flawfinder    | CWE-mapped vulnerability scanning with severity levels      |
| `DeadCodeAdapter`        | CCLS/libclang | BFS reachability analysis from entry points                 |
| `CallGraphAdapter`       | CCLS/libclang | Fan-in/fan-out, cycle detection, max call depth             |
| `FunctionMetricsAdapter` | CCLS/libclang | Function body lines, parameters, templates, virtuals        |
| `ExcelReportAdapter`     | ExcelWriter   | Generates `static_*` prefixed tabs in Excel output          |

All adapters inherit from `BaseStaticAdapter` and degrade gracefully when their underlying tool is unavailable.

### Project Layout

```text
.
â”œâ”€â”€ main.py                             # Entry point & LangGraph workflow
â”œâ”€â”€ fixer_workflow.py                   # Human-in-the-loop repair workflow
â”œâ”€â”€ global_config.yaml                  # Hierarchical YAML configuration
â”œâ”€â”€ requirements.txt                    # Python dependencies
â”œâ”€â”€ agents/
â”‚   â”œâ”€â”€ codebase_static_agent.py        # Unified 7-phase static analyzer
â”‚   â”œâ”€â”€ codebase_llm_agent.py           # LLM-exclusive per-file code reviewer
â”‚   â”œâ”€â”€ codebase_fixer_agent.py         # Agentic code repair agent (source-aware, audit trail)
â”‚   â”œâ”€â”€ codebase_patch_agent.py         # Patch analysis agent (diff-based issue detection)
â”‚   â”œâ”€â”€ codebase_analysis_chat_agent.py # Interactive chat analysis agent
â”‚   â”œâ”€â”€ adapters/                       # Deep static analysis adapters
â”‚   â”‚   â”œâ”€â”€ base_adapter.py             #   ABC base class
â”‚   â”‚   â”œâ”€â”€ ast_complexity_adapter.py   #   Lizard integration
â”‚   â”‚   â”œâ”€â”€ security_adapter.py         #   Flawfinder integration
â”‚   â”‚   â”œâ”€â”€ dead_code_adapter.py        #   CCLS dead code detection
â”‚   â”‚   â”œâ”€â”€ call_graph_adapter.py       #   CCLS call graph analysis
â”‚   â”‚   â”œâ”€â”€ function_metrics_adapter.py #   CCLS function metrics
â”‚   â”‚   â””â”€â”€ excel_report_adapter.py     #   static_ Excel tab generator
â”‚   â”œâ”€â”€ context/                        # Header context injection for LLM analysis
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ header_context_builder.py   #   Include resolution, header parsing, context assembly
â”‚   â”œâ”€â”€ analyzers/                      # 9 regex-based health analyzers
â”‚   â”‚   â”œâ”€â”€ base_runtime_analyzer.py    #   ABC base for all analyzers
â”‚   â”‚   â”œâ”€â”€ complexity_analyzer.py
â”‚   â”‚   â”œâ”€â”€ security_analyzer.py
â”‚   â”‚   â”œâ”€â”€ dependency_analyzer.py
â”‚   â”‚   â”œâ”€â”€ memory_corruption_analyzer.py
â”‚   â”‚   â”œâ”€â”€ null_pointer_analyzer.py
â”‚   â”‚   â”œâ”€â”€ potential_deadlock_analyzer.py
â”‚   â”‚   â”œâ”€â”€ quality_analyzer.py
â”‚   â”‚   â”œâ”€â”€ maintainability_analyzer.py
â”‚   â”‚   â”œâ”€â”€ documentation_analyzer.py
â”‚   â”‚   â””â”€â”€ test_coverage_analyzer.py
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ file_processor.py           # File discovery & caching
â”‚   â”‚   â””â”€â”€ metrics_calculator.py       # Orchestrates analyzers + adapters
â”‚   â”œâ”€â”€ prompts/
â”‚   â”‚   â””â”€â”€ prompts.py                  # PromptTemplates for LLM agents
â”‚   â”œâ”€â”€ parsers/
â”‚   â”‚   â”œâ”€â”€ excel_to_agent_parser.py    # Excel â†’ JSONL directives parser
â”‚   â”‚   â”œâ”€â”€ healthreport_generator.py   # HTML health report renderer
â”‚   â”‚   â””â”€â”€ healthreport_parser.py      # Legacy health report parser
â”‚   â”œâ”€â”€ visualization/
â”‚   â”‚   â””â”€â”€ graph_generator.py          # Dependency graph visualization
â”‚   â””â”€â”€ vector_db/
â”‚       â””â”€â”€ document_processor.py       # Vector document processing
â”œâ”€â”€ db/
â”‚   â”œâ”€â”€ postgres_db_setup.py            # PostgreSQL schema setup (vector + telemetry + HITL)
â”‚   â”œâ”€â”€ postgres_api.py                 # PostgreSQL API helpers
â”‚   â”œâ”€â”€ telemetry_service.py            # Silent telemetry collector (TelemetryService)
â”‚   â”œâ”€â”€ schema_telemetry.sql            # SQL schema for telemetry & HITL tables
â”‚   â”œâ”€â”€ json_flattner.py                # JSON â†’ flat JSON converter
â”‚   â”œâ”€â”€ ndjson_processor.py             # NDJSON processor for embeddings
â”‚   â”œâ”€â”€ ndjson_writer.py                # NDJSON file writer
â”‚   â”œâ”€â”€ vectordb_pipeline.py            # Vector DB ingestion pipeline
â”‚   â””â”€â”€ vectordb_wrapper.py             # Vector DB abstraction wrapper
â”œâ”€â”€ dependency_builder/                 # CCLS / libclang integration
â”‚   â”œâ”€â”€ ccls_code_navigator.py          # LSP-based code navigation
â”‚   â”œâ”€â”€ ccls_ingestion.py               # CCLS indexing orchestrator
â”‚   â”œâ”€â”€ ccls_dependency_builder.py      # Dependency graph builder
â”‚   â”œâ”€â”€ dependency_service.py           # Dependency resolution service
â”‚   â”œâ”€â”€ dependency_handler.py           # Dependency processing handler
â”‚   â”œâ”€â”€ connection_pool.py              # LSP connection pooling
â”‚   â”œâ”€â”€ config.py                       # DependencyBuilderConfig
â”‚   â”œâ”€â”€ models.py                       # Data models
â”‚   â”œâ”€â”€ metrics.py                      # Performance metrics
â”‚   â”œâ”€â”€ lsp_notification_handlers.py    # CCLS LSP notification handlers
â”‚   â”œâ”€â”€ exceptions.py                   # Custom exceptions
â”‚   â””â”€â”€ utils.py                        # Shared utilities
â”œâ”€â”€ hitl/                               # Human-in-the-Loop RAG pipeline
â”‚   â”œâ”€â”€ __init__.py                     # Module exports, HITL_AVAILABLE flag
â”‚   â”œâ”€â”€ config.py                       # HITLConfig dataclass (PostgreSQL connection)
â”‚   â”œâ”€â”€ schemas.py                      # Data models (FeedbackDecision, ConstraintRule)
â”‚   â”œâ”€â”€ feedback_store.py               # PostgreSQL persistent store (migrated from SQLite)
â”‚   â”œâ”€â”€ excel_feedback_parser.py        # Parse Excel human feedback
â”‚   â”œâ”€â”€ constraint_parser.py            # Parse *_constraints.md files
â”‚   â”œâ”€â”€ rag_retriever.py                # RAG query engine
â”‚   â”œâ”€â”€ hitl_context.py                 # Unified agent interface (HITLContext)
â”‚   â””â”€â”€ prompts.py                      # RAG-augmented prompt templates
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ common/
â”‚   â”‚   â”œâ”€â”€ llm_tools.py                # Multi-provider LLM abstraction
â”‚   â”‚   â”œâ”€â”€ excel_writer.py             # Excel report generator
â”‚   â”‚   â”œâ”€â”€ email_reporter.py           # SMTP email reporter
â”‚   â”‚   â””â”€â”€ mmdtopdf.py                # Mermaid â†’ PDF converter
â”‚   â”œâ”€â”€ parsers/
â”‚   â”‚   â”œâ”€â”€ env_parser.py               # .env / environment config
â”‚   â”‚   â””â”€â”€ global_config_parser.py     # YAML GlobalConfig parser
â”‚   â””â”€â”€ prompts/
â”‚       â””â”€â”€ prompts.py                  # Utility prompt helpers
â”œâ”€â”€ prompts/
â”‚   â””â”€â”€ codebase_analysis_prompt.py     # LLM analysis prompt template
â””â”€â”€ ui/
    â”œâ”€â”€ app.py                          # Main Streamlit app (7 workflow tabs)
    â”œâ”€â”€ streamlit_tools.py              # Custom Streamlit helpers (sidebar, themes)
    â”œâ”€â”€ background_workers.py           # Thread runners with telemetry instrumentation
    â”œâ”€â”€ feedback_helpers.py             # Excel feedback persistence helpers
    â”œâ”€â”€ qa_inspector.py                 # QA validation inspector
    â””â”€â”€ launch_streamlit.py             # Streamlit launcher
```

---

## Installation & Setup

### 1. System Prerequisites

**Python 3.12+** is recommended.



**Install PostgreSQL and initialize the database** (required for vector DB pipeline):

**macOS / Linux (Option A â€” automated bootstrap):**
```bash
sudo ./bootstrap_db.sh
```
The script auto-detects the installed PostgreSQL version, installs pgvector (building from source if the Homebrew bottle doesn't match your PG version), creates the user, database, extension, and permissions.

**macOS (Option B â€” manual Homebrew setup):**
```bash
# Install PostgreSQL and start the service
brew install postgresql@16
brew services start postgresql@16

# Install pgvector (build from source if brew bottle doesn't cover your PG version)
brew install pgvector
# If CREATE EXTENSION fails later, build from source:
cd /tmp && git clone --branch v0.8.1 --depth 1 https://github.com/pgvector/pgvector.git
cd pgvector && PG_CONFIG=/opt/homebrew/opt/postgresql@16/bin/pg_config make && sudo make install

# Create the database, user, and extension
/opt/homebrew/opt/postgresql@16/bin/psql -d postgres -c "CREATE USER codebase_analytics_user WITH PASSWORD 'postgres';"
/opt/homebrew/opt/postgresql@16/bin/psql -d postgres -c "CREATE DATABASE codebase_analytics_db OWNER codebase_analytics_user;"
/opt/homebrew/opt/postgresql@16/bin/psql -d codebase_analytics_db -c "CREATE EXTENSION IF NOT EXISTS vector;"
```

**Windows (PowerShell â€” run as Administrator):**
```powershell
.\bootstrap_db.ps1
```

All options install PostgreSQL with pgvector, create the application user and database, enable the vector extension, and grant all required permissions. Defaults match `global_config.yaml`. Override with environment variables if needed:

## Optional
```bash
# macOS / Linux
DB_USER=myuser DB_PASSWORD=mypass DB_NAME=mydb sudo -E ./bootstrap_db.sh

# Windows PowerShell
$env:DB_USER="myuser"; $env:DB_PASSWORD="mypass"; $env:DB_NAME="mydb"; .\bootstrap_db.ps1
```

### 2. Python Environment Setup

```bash
sudo su
```

```bash
# if there is already an .env
deactivate
rm -rf .venv

# Create and activate virtual environment
python -m venv .venv
source .venv/bin/activate
python --version
```

### 3. Install Dependencies

```bash
pip install --upgrade pip

pip install qgenie-sdk[all] qgenie-sdk-tools -i https://devpi.qualcomm.com/qcom/dev/+simple --trusted-host devpi.qualcomm.com
pip install -r requirements.txt
```

**Install CCLS** (required for `dependency_builder` and deep adapters):

```bash
# macOS
brew install ccls

# Ubuntu/Debian
sudo apt-get update && sudo apt-get install -y ccls

# snap alternative
sudo snap install ccls --classic

# Windows (via Chocolatey)
choco install ccls
```

### install ccls and clang 
```bash
sudo apt install clang-14 libclang-14-dev llvm-14-dev
export CC=clang-14
export CXX=clang++-14

#verify
echo $CC
echo $CXX
```

### 4. Configuration

Copy and customize the global configuration:

```bash
cp global_config.yaml.example global_config.yaml
# OR edit global_config.yaml directly
```

Set your LLM provider in `global_config.yaml`:

```yaml
llm:
  model: anthropic::claude-sonnet-4-20250514   # Anthropic Claude
  # model: qgenie::qwen2.5-14b-1m              # QGenie
  # model: vertexai::gemini-2.5-pro             # Google Vertex AI
  # model: azure::gpt-4.1                       # Azure OpenAI
```

Set API keys via environment or `.env`:

```bash
cp env.example .env
# Edit .env:
#   ANTHROPIC_API_KEY=sk-...
#   QGENIE_API_KEY=...
#   POSTGRES_PASSWORD=...
```

## ğŸ›¡ï¸ Constraints & Rules Engine

The CURE ecosystem uses a **Context-Aware Constraints System**. This allows you to inject domain-specific knowledge, hardware limitations, and coding standards into the LLM agents without modifying the source code.

Constraints are defined in Markdown files and are split into two specific categories:
1.  **Issue Identification Rules:** Tell the Analysis Agent what to **IGNORE** (False Positive filtering).
2.  **Issue Resolution Rules:** Tell the Fixer Agent **HOW TO FIX** the code (Performance/Style guidelines).

### ğŸ“‚ 1. Directory Structure

All constraint files must be placed in the **`agents/constraints/`** directory.

| Scope        | File Naming Convention      | Description                                                                                                             |
| :----------- | :-------------------------- | :---------------------------------------------------------------------------------------------------------------------- |
| **Global**   | `common_constraints.md`     | Rules applied to **ALL** files during analysis or fixing. Use this for project-wide standards.                          |
| **Specific** | `<filename>_constraints.md` | Rules applied **ONLY** to a specific source file.<br>_Example:_ `dp_main.c_constraints.md` applies only to `dp_main.c`. |

### ğŸ“ 2. File Format & Required Headers

The agents use **Regex Parsing** to extract rules. You **MUST** use the specific Markdown headers below for the system to recognize your instructions.

#### A. The Identification Section
*   **Target Agent:** `CodebaseLLMAgent`, `CodebasePatchAgent`
*   **Required Header:** `## Issue Identification Rules`
*   **Purpose:** To prevent the LLM from flagging known safe patterns (e.g., "Hardware registers are never NULL").

#### B. The Resolution Section
*   **Target Agent:** `CodebaseFixerAgent`
*   **Required Header:** `## Issue Resolution Rules`
*   **Purpose:** To guide the LLM on how to generate code (e.g., "Do not use Mutex in ISR", "Use `memcpy` instead of `memcpy_s`").

### ğŸ’¡ 3. Constraint File Template

Create a file (e.g., `agents/constraints/my_driver.c_constraints.md`) using this structure:

```markdown
# Constraints for my_driver.c

## 1. Issue Identification Rules (WHAT TO IGNORE)
*Use these rules to filter out False Positives during analysis.*

### A. Hardware Contexts
*   **Rule**: Variables `soc`, `pdev`, `hif_ctx` are initialized at boot. **IGNORE** missing NULL checks in data paths.
*   **Rule**: **IGNORE** array bounds checks if the index is `loop_counter % MAX_HW_QUEUES`.

---

## 2. Issue Resolution Rules (HOW TO FIX)
*Use these rules when generating code fixes.*

### A. Critical Performance
*   **Rule**: **DO NOT** introduce locking (mutex/spinlocks) in `isr_handler` functions.
*   **Rule**: **RETAIN** `memcpy` for performance. Do not replace with `memcpy_s`. Instead, add an explicit `if (len > size)` check before the call.

### B. Error Handling
*   **Rule**: In `void` functions, prefer `return;` over complex error handling. Do not log to serial console in hot paths.
```

### âš™ï¸ 4. Workflow Integration

| Agent            | Input Section          | Behavior                                                                                                        |
| :--------------- | :--------------------- | :-------------------------------------------------------------------------------------------------------------- |
| **LLM Analysis** | `Identification Rules` | Injects rules into the system prompt to filter out False Positives before they are reported.                    |
| **Patch Agent**  | `Identification Rules` | Uses these rules to analyze diffs, ensuring that new code is checked against project-specific ignore lists.     |
| **Fixer Agent**  | `Resolution Rules`     | Injects rules into the code generation prompt to ensure fixes comply with performance budgets and style guides. |
--------------------------------------------------------------------------------------------------------------------------------------------------------------

## Usage

### CLI Reference

| Flag                            | Description                                                               |
| :------------------------------ | :------------------------------------------------------------------------ |
| `--codebase-path PATH`          | Path to the C/C++ codebase to analyze                                     |
| `-d, --out-dir DIR`             | Output directory for generated files (default: `./out`)                   |
| `--config-file PATH`            | Path to `global_config.yaml` (default: auto-detected)                     |
| `--use-llm`                     | Enable LLM enrichment in StaticAnalyzerAgent (health report mode)         |
| `--llm-exclusive`               | Use CodebaseLLMAgent exclusively for Excel report (skips health pipeline) |
| `--enable-adapters`             | Run deep static analysis(Lizard, Flawfinder, CCLS) --llm-exclusive requ   |
| `--use-ccls`                    | Enable CCLS dependency services for CodebaseLLMAgent                      |
| `--file-to-fix FILE`            | Analyze a specific file (relative to codebase path)                       |
| `--llm-model MODEL`             | LLM model in `provider::model` format (overrides config)                  |
| `--llm-api-key KEY`             | API key for the LLM provider                                              |
| `--llm-max-tokens N`            | Max tokens per LLM request (default: 15000)                               |
| `--llm-temperature F`           | LLM temperature (default: 0.1)                                            |
| `--max-files N`                 | Max files to analyze (default: 2000)                                      |
| `--batch-size N`                | Files per analysis batch (default: 25)                                    |
| `--exclude-dirs D [D]`          | Directories to exclude from analysis                                      |
| `--exclude-globs G [G]`         | Glob patterns to exclude (e.g., `*.test.cpp`)                             |
| `--enable-vector-db`            | Enable vector DB ingestion pipeline                                       |
| `--vector-chunk-size N`         | Chunk size for vector embeddings (default: 4000)                          |
| `--vector-overlap-size N`       | Overlap between chunks (default: 200)                                     |
| `--vector-include-code`         | Include source code in vector embeddings (default: on)                    |
| `--enable-chatbot-optimization` | Enable chatbot-optimized vector processing                                |
| `--generate-report`             | Generate HTML health report from healthreport.json                        |
| `--generate-visualizations`     | Generate dependency graph visualizations                                  |
| `--generate-pdfs`               | Generate PDF outputs from Mermaid diagrams                                |
| `--max-edges N`                 | Max edges in graph visualizations (default: 500)                          |
| `--health-report-path PATH`     | Override path for healthreport.json output                                |
| `--flat-json-path PATH`         | Override path for flattened JSON output                                   |
| `--ndjson-path PATH`            | Override path for NDJSON output                                           |
| `--force-reanalysis`            | Force re-analysis ignoring cached results                                 |
| `--memory-limit MB`             | Memory limit in MB (default: 3000)                                        |
| `--enable-memory-monitoring`    | Enable real-time memory monitoring (default: on)                          |
| `--patch-file PATH`             | Path to `.patch`/`.diff` file for patch analysis                          |
| `--patch-target PATH`           | Path to the original source file being patched                            |
| `--enable-hitl`                 | Enable Human-in-the-Loop RAG feedback system                              | `false`                  |
| `--hitl-feedback-excel`         | Path to `detailed_code_review.xlsx` with human feedback                   | `None`                   |
| `--hitl-constraints-dir`        | Directory to search for `*_constraints.md` files                          | `None`                   |
| `--hitl-store-path`             | Legacy (deprecated) â€” HITL now uses PostgreSQL                            | *(ignored)*              |
| `-v, --verbose`                 | Verbose logging                                                           |
| `-D, --debug`                   | Debug logging                                                             |
| `--quiet`                       | Suppress non-error output                                                 |

#### Fixer Workflow CLI (`fixer_workflow.py`)

| Flag                                  | Description                                                                                              |
| :------------------------------------ | :------------------------------------------------------------------------------------------------------- |
| `--excel-file PATH`                   | Path to the reviewed Excel file (default: `out/detailed_code_review.xlsx`)                               |
| `--codebase-path PATH`                | Root directory of the source code                                                                        |
| `--out-dir DIR`                       | Directory for backups/intermediate files                                                                 |
| `--fix-source {all,llm,static,patch}` | Process only issues from: all, llm (Analysis sheet), static (static_* sheets), or patch (patch_* sheets) |
| `--llm-model MODEL`                   | LLM model in `provider::model` format                                                                    |
| `--dry-run`                           | Simulate fixes without writing to disk                                                                   |

### Standard Analysis (Health Report Pipeline)

```bash
# Basic static analysis (fast, regex-based)
python main.py --codebase-path /path/to/cpp/project

# With LLM enrichment
python main.py --codebase-path /path/to/cpp/project --use-llm

# With deep static adapters (Lizard + Flawfinder + CCLS) - --llm-exclusive is required to generate xlsx.
python main.py --codebase-path /path/to/cpp/project --enable-adapters --llm-exclusive

# Full pipeline with vector DB
python main.py --codebase-path /path/to/cpp/project \
  --use-llm --enable-adapters --enable-vector-db --generate-report
```

### Exclusive LLM Analysis (Direct Excel Report)

This mode skips the LangGraph health pipeline and generates `detailed_code_review.xlsx` directly.

```bash
# LLM-only analysis
python main.py --llm-exclusive --codebase-path /path/to/cpp/project

# LLM + deep adapters (static_ tabs merged into same Excel)
python main.py --llm-exclusive --enable-adapters --codebase-path /path/to/cpp/project

# With CCLS dependency context
python main.py --llm-exclusive --enable-adapters --use-ccls \
  --codebase-path /path/to/cpp/project

# Targeted single-file analysis
python main.py --llm-exclusive --use-ccls \
  --file-to-fix "src/module/component.cpp" \
  --codebase-path /path/to/cpp/project
```

### LLM Provider Selection

```bash
# Anthropic Claude
python main.py --llm-exclusive --llm-model "anthropic::claude-sonnet-4-20250514" \
  --codebase-path /path/to/project

# Google Vertex AI
python main.py --llm-exclusive --llm-model "vertexai::gemini-2.5-pro" \
  --codebase-path /path/to/project

# Azure OpenAI
python main.py --llm-exclusive --llm-model "azure::gpt-4.1" \
  --codebase-path /path/to/project
```

### Streamlit Dashboard

```bash
python -m streamlit run ui/app.py --server.port 8502
```

Access at: `http://localhost:8502`

The UI provides seven workflow tabs: **Analyze**, **Pipeline**, **Review**, **Fix & QA**, **Audit**, **Constraints**, and **Telemetry**. The sidebar includes toggles for enabling HITL and Telemetry.

---

## Agentic Code Repair (Human-in-the-Loop)

The pipeline includes a **CodebaseFixerAgent** that closes the loop between analysis and remediation.

### Workflow

1. **Analyze**: Run `main.py` to generate `detailed_code_review.xlsx`.
2. **Human Review**: Open the Excel, review High/Critical issues, add feedback and constraints.
3. **Execute Fixes**: Run `fixer_workflow.py` to apply LLM-guided fixes.

### Fixer Commands

```bash
# Single-step (parse + fix)
python fixer_workflow.py --excel detailed_code_review.xlsx --codebase-path /path/to/project

# Parse Excel to JSONL only
python fixer_workflow.py --step parse --excel detailed_code_review.xlsx

# Run the fixer agent only
python fixer_workflow.py --step fix --codebase-path /path/to/project
```

### Feedback & Constraints Columns

**Feedback column** â€” controls the action:

| User Input                    | Effect                                                |
| :---------------------------- | :---------------------------------------------------- |
| *(Empty)*                     | **Approve.** Apply `Fixed_Code` as suggested.         |
| `Skip` / `Ignore` / `No Fix`  | **Reject.** File untouched (false positive).          |
| `Approved` / `LGTM`           | **Approve.** Explicit confirmation.                   |
| `Modify` / `Update` / `Retry` | **Custom fix.** Re-generate using Constraints column. |

**Constraints column** â€” provides technical guardrails for custom fixes, for example: "Use `std::array` instead of `std::vector`", "Follow C++98 only", "Wrap in `std::lock_guard`", etc.

### Fixer Features

- **Holistic Refactoring**: Fixes multiple issues in a single file simultaneously for consistency.
- **Smart Backups**: Creates a mirror in `out/shelved_backups` before modifying any file.
- **Safety Gates**: Checks for file size anomalies and LLM failures before overwriting.
- **Audit Reporting**: Produces `final_execution_audit.xlsx` with color-coded status (FIXED, SKIPPED, FAILED).

---

## Configuration Reference

### global_config.yaml

The `global_config.yaml` file provides hierarchical, typed configuration with `${ENV_VAR}` override support. Key sections:

| Section              | Purpose                                               |
| :------------------- | :---------------------------------------------------- |
| `paths`              | Input/output directories, prompt file paths           |
| `llm`                | Provider, model, API keys, token limits, temperature  |
| `embeddings`         | Vector embedding model selection                      |
| `database`           | PostgreSQL connection, PGVector collection settings   |
| `email`              | SMTP report delivery configuration                    |
| `dependency_builder` | CCLS executable, timeouts, BFS depth, connection pool |
| `excel`              | Report styling (colors, column widths, freeze/filter) |
| `mermaid`            | Diagram rendering configuration                       |
| `hitl`               | HITL RAG pipeline â€” feedback store, constraint parsing |
| `context`            | Header context injection â€” include paths, depth, token budget |
| `telemetry`          | Silent usage telemetry (enable/disable)               |
| `logging`            | Log level, verbose/debug flags                        |

---

## Telemetry & Analytics

CURE includes a silent, fire-and-forget telemetry system that records framework usage patterns into the same PostgreSQL database (`codebase_analytics_db`). Telemetry is enabled by default and can be toggled in `global_config.yaml` or the Streamlit sidebar.

### What is tracked

- **Run summaries**: mode (analysis/fixer/patch), status, duration, file count, issue counts by severity
- **Fix outcomes**: issues fixed, skipped, and failed per fixer run
- **LLM usage**: provider, model, token counts (prompt + completion), latency per call
- **Granular events**: individual issue found/fixed/skipped events, phase transitions, export actions

### Configuration

```yaml
# global_config.yaml
telemetry:
  enable: true   # set to false to disable all telemetry
```

### Database tables

| Table                | Purpose                                           |
| :------------------- | :------------------------------------------------ |
| `telemetry_runs`     | One row per analysis/fixer/patch run              |
| `telemetry_events`   | Granular events within a run (issues, LLM calls)  |

Tables are auto-created by `PostgresDbSetup` during database initialization, or can be manually applied via `db/schema_telemetry.sql`.

### Dashboard

The **Telemetry** tab in the Streamlit UI displays: total runs, issues found/fixed, fix success rate, runs over time, issues by severity, top issue types, LLM usage by model, and a drill-down into individual run events.

---

## Human-in-the-Loop (HITL) Feedback Store

The HITL pipeline uses **PostgreSQL** (shared `codebase_analytics_db`) for persistent storage of human feedback decisions and constraint rules. This enables agents to learn from accumulated human review history across runs.

### Database tables

| Table                       | Purpose                                     |
| :-------------------------- | :------------------------------------------ |
| `hitl_feedback_decisions`   | Human review outcomes (SKIP, FIX, etc.)     |
| `hitl_constraint_rules`     | Parsed constraint rules from markdown files |
| `hitl_run_metadata`         | Audit trail of analysis runs                |

### Enabling HITL

HITL can be enabled via the CLI (`--enable-hitl`), `global_config.yaml` (`hitl.enable: true`), or the **Enable HITL** toggle in the Streamlit sidebar. When enabled, agents will check past feedback decisions and inject constraint-aware context into LLM prompts.

---

## Troubleshooting

### Memory Optimization

```bash
# Memory-optimized run for large codebases
python main.py --codebase-path ./codebase \
  --max-files 1000 --batch-size 50 --memory-limit 3000

# Debug mode with monitoring
python main.py --debug --enable-memory-monitoring --max-files 500
```

### Database Maintenance

```bash
psql -h localhost -U codebase_analytics_user -d codebase_analytics_db
```

```sql
-- Check embeddings
SELECT document, cmetadata FROM langchain_pg_embedding LIMIT 5;

-- Check telemetry runs
SELECT run_id, mode, status, issues_total, issues_fixed, duration_seconds
FROM telemetry_runs ORDER BY created_at DESC LIMIT 10;

-- Check HITL feedback history
SELECT issue_type, human_action, COUNT(*) FROM hitl_feedback_decisions
GROUP BY issue_type, human_action ORDER BY count DESC;

-- Clear all vector data
DELETE FROM langchain_pg_embedding;
DELETE FROM langchain_pg_collection;
```

### Common Issues

- **`ModuleNotFoundError: No module named 'networkx'`**: Run `pip install -r requirements.txt` to install all dependencies.
- **Adapters show "tool not available"**: Install optional tools â€” `pip install lizard flawfinder`. For CCLS adapters, ensure `ccls` is installed and in PATH.
- **CCLS indexing timeout**: Increase `dependency_builder.indexing_timeout_seconds` in `global_config.yaml`.
- **LLM provider errors**: Verify `--llm-model` uses the correct `provider::model` format and that the corresponding API key is set.

---

## General debug - find dependency issues
``` bash
 
 python -c 'import sys; sys.path.append("."); from utils.common.email_reporter import EmailReporter; print("Success!")'
 python -c 'import sys; sys.path.append("."); from utils.common.excel_writer import ExcelWriter; print("Success!")'
 python -c 'import sys; sys.path.append("."); from utils.common.llm_tools import LLMTools; print("Success!")'
 

  export LIBCLANG_PATH=/opt/homebrew/Cellar/llvm/21.1.8/lib/libclang.dylib 
```


For Error initializing libclang: /usr/lib/llvm-14/lib/libclang.so: undefined symbol: clang_CXXMethod_isDeleted. Please ensure that your python bindings are compatible with your libclang.so version.

``` bash 
pip uninstall -y clang libclang
pip install clang==14.*
``
 

 ```

## Context-Aware LLM Analysis (Header Context Injection)

CURE's LLM-exclusive analysis now automatically resolves `#include` directives and injects relevant type definitions from header files into each code chunk sent to the LLM. This significantly reduces false positives by giving the LLM visibility into enum ranges, macro constants, struct layouts, typedefs, and function signatures that were previously invisible.

### Problem

Without header context, the LLM frequently flags valid code as problematic because it cannot see definitions from included headers. Common false positives include enum-bounded array accesses flagged as out-of-bounds, macro-defined buffer sizes flagged as unchecked, struct field accesses flagged as invalid, and known function return types misinterpreted.

### How It Works

The `HeaderContextBuilder` module (`agents/context/header_context_builder.py`) operates in three phases:

1. **Include Resolution**: Parses `#include` directives from the source file and recursively resolves them to actual header file paths (configurable depth, default 2 levels). System headers (`<stdio.h>`, etc.) are excluded by default since LLMs already understand standard library types.

2. **Header Parsing**: Extracts definitions from each resolved header using regex patterns â€” enums (with member values and auto-increment tracking), structs/unions (with field types and array sizes), `#define` macros (with numeric value evaluation), typedefs, function prototypes, and extern variable declarations. Results are cached per header file for the entire analysis run.

3. **Relevance Filtering**: For each code chunk, the builder identifies which definitions are actually referenced (via identifier matching) and assembles a concise context string. A priority system (enums > macros > structs > typedefs > protos > externs) ensures the most impactful definitions fit within the configurable token budget.

### What Gets Injected

The context is injected above each code chunk in the LLM prompt:

```c
// â”€â”€â”€â”€ HEADER CONTEXT (from included headers) â”€â”€â”€â”€
// Enums:
enum wifi_band { WIFI_BAND_2G = 0, WIFI_BAND_5G = 1, WIFI_BAND_6G = 2, WIFI_BAND_MAX = 3 };

// Macros:
#define MAX_CHANNELS 64
#define BUF_SIZE 4096

// Structs:
struct channel_info { uint8_t band; uint16_t freq; int8_t power; uint32_t flags; };

// Function prototypes:
int wifi_validate_channel(struct channel_info *info, enum wifi_band band);
// â”€â”€â”€â”€ END HEADER CONTEXT â”€â”€â”€â”€
```

### False Positive Categories Addressed

| Pattern | Root Cause | How Context Fixes It |
|:--------|:-----------|:---------------------|
| Enum-indexed array flagged as OOB | LLM cannot see enum range | Enum definition shows MAX value matches array size |
| Macro-bounded buffer flagged as unchecked | LLM cannot see `#define` value | Macro injection shows numeric constant |
| Struct field access flagged as invalid | LLM cannot see struct layout | Struct definition shows valid fields |
| `sizeof(struct)` flagged as wrong | LLM does not know struct size | Full struct layout provided |
| Function return used without NULL check | LLM does not know return type | Prototype shows `int` return (non-pointer) |
| Typedef'd type misunderstood | LLM does not know underlying type | `typedef uint32_t status_t;` resolves ambiguity |
| `ARRAY_SIZE` macro not recognized | LLM sees unknown macro | Macro definition injected |
| Conditional compilation flagged as dead code | LLM does not understand `#ifdef` | Prompt rules cover this pattern |
| Bit flags used as array indices | LLM confuses flags with indices | Enum with hex values shows bit flag pattern |

### Configuration

Add to `global_config.yaml` (enabled by default):

```yaml
context:
  enable_header_context: true
  include_paths: []              # Additional -I style paths (relative to codebase root)
  max_header_depth: 2            # How deep to follow #include chains (0 = direct only)
  max_context_chars: 6000        # Max chars for header context per chunk (~1500 tokens)
  exclude_system_headers: true   # Skip <stdio.h>, <stdlib.h>, etc.
```

### Key Design Decisions

- **No CCLS required**: Works entirely via regex-based parsing. When CCLS is also enabled, both context sources complement each other (CCLS provides call graphs, HeaderContext provides type definitions).
- **Cached per run**: Each header is parsed once and reused across all files and chunks in the analysis run.
- **Token budget aware**: The `max_context_chars` limit ensures header context does not consume too much of the LLM's input window. Priority ordering ensures the most impactful definitions (enums, macros) are included first.
- **Backward compatible**: If disabled or if the module fails to import, the pipeline runs exactly as before.

### Files

```text
agents/context/
â”œâ”€â”€ __init__.py
â””â”€â”€ header_context_builder.py    # Include resolution, header parsing, context assembly
```

Modified: `agents/codebase_llm_agent.py` (integration), `prompts/codebase_analysis_prompt.py` (9 context-aware rules), `global_config.yaml` (configuration).

---

## Contributing

Contributions are welcome! Please open issues and pull requests for any improvements or bug fixes.

## License

This project is licensed under the [MIT License](LICENSE).
# CURE
